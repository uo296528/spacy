{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32ad051",
   "metadata": {},
   "source": [
    "# Ejercicio: Fragmentador de capítulos en un documento\n",
    "\n",
    "Define extensiones personalizadas para los objetos `Span` y `Doc`:\n",
    "\n",
    "- Cada `Span` (capítulo) debe tener:\n",
    "  - Su número (`numero`)\n",
    "  - Su título (`titulo`)\n",
    "  - Su longitud en tokens (`longitud`)\n",
    "\n",
    "- El objeto `Doc` debe tener:\n",
    "  - Una lista de todos los capítulos (`capitulos`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8174e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"libro1.txt\", \"r\", encoding=\"utf-8\") as archivo:\n",
    "    lineas = archivo.readlines()[37:]  # Desde la línea 38\n",
    "    texto = \"\".join(lineas)\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "doc = nlp(texto)\n",
    "#importa la clase Span\n",
    "from spacy.tokens import Span, Doc\n",
    "#hagamos un span para cada capítulo. debe tener su número, su título y su longitud\n",
    "if not Span.has_extension(\"numero\"):\n",
    "    # Registrar atributos personalizados en Span\n",
    "    Span.set_extension(\"numero\", default=None)\n",
    "    '''Span.set_extension(\"titulo\", default=None)\n",
    "    Span.set_extension(\"longitud\", default=None)\n",
    "# Registrar atributos personalizados en Span\n",
    "Span.set_extension(\"numero\", default=None)'''\n",
    "if not Span.has_extension(\"titulo\"):\n",
    "    # Registrar atributos personalizados en Span\n",
    "    Span.set_extension(\"titulo\", default=None)\n",
    "if not Span.has_extension(\"longitud\"):\n",
    "    # Registrar atributos personalizados en Span\n",
    "    Span.set_extension(\"longitud\", default=None)\n",
    "#Buscar encabezados de capítulo con regex: número al principio de línea seguido de texto\n",
    "# Ejemplo: \"1 El niño que vivió\"\n",
    "capitulos = list(re.finditer(r\"(?m)^(\\d+)\\s+([A-Z][^\\n]*)\", texto))\n",
    "\n",
    "# Lista para guardar la info de cada capítulo\n",
    "capitulos_info = []\n",
    "\n",
    "# Iterar por capítulos para construir spans\n",
    "for i, match in enumerate(capitulos):\n",
    "    numero = int(match.group(1))\n",
    "    titulo = match.group(2).strip()\n",
    "\n",
    "    # Índices del capítulo actual en caracteres\n",
    "    start_char = match.end()\n",
    "\n",
    "    # Determinar el final del capítulo: inicio del siguiente o final del texto\n",
    "    if i + 1 < len(capitulos):\n",
    "        end_char = capitulos[i + 1].start()\n",
    "    else:\n",
    "        end_char = len(texto)\n",
    "\n",
    "    # Crear el span usando char_span\n",
    "    span = doc.char_span(start_char, end_char, alignment_mode=\"expand\")\n",
    "\n",
    "    if span:\n",
    "        # Atributos personalizados\n",
    "        span._.numero = numero\n",
    "        span._.titulo = titulo\n",
    "        span._.longitud = len(span)\n",
    "\n",
    "        capitulos_info.append(span)\n",
    "    \n",
    "resumen = \"\\n\".join([\n",
    "    f\"{span._.numero} {span._.titulo} {span._.longitud}\"\n",
    "    for span in capitulos_info\n",
    "])\n",
    "resumen\n",
    "\n",
    "\n",
    "# Crear un nuevo Doc con ese texto\n",
    "doc_resumen = nlp.make_doc(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9fcbc",
   "metadata": {},
   "source": [
    "Crea un componente personalizado para añadir luego al pipeline, llamado `fragmentador_capitulos` que:\n",
    "\n",
    "- Reciba un `Doc`\n",
    "- Busque los capítulos utilizando una expresión regular, donde cada capítulo esté definido por un número seguido de su título.\n",
    "- Cree un `Span` para cada capítulo con los atributos personalizados definidos.\n",
    "- Guarde la lista de capítulos en el atributo `doc._.capitulos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b074dfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.fragmentador_capitulos(doc)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component(\"fragmentador_capitulos\")\n",
    "def fragmentador_capitulos(doc):\n",
    "    texto = doc.text\n",
    "\n",
    "    # Buscar encabezados de capítulos: número + título\n",
    "    capitulos = list(re.finditer(r\"(?m)^(\\d+)\\s+([A-Z][^\\n]*)\", texto))\n",
    "    spans = []\n",
    "\n",
    "    for i, match in enumerate(capitulos):\n",
    "        numero = int(match.group(1))\n",
    "        titulo = match.group(2).strip()\n",
    "        start_char = match.end()\n",
    "        end_char = capitulos[i + 1].start() if i + 1 < len(capitulos) else len(texto)\n",
    "\n",
    "        # Crear Span desde el texto original\n",
    "        span = doc.char_span(start_char, end_char, alignment_mode=\"expand\")\n",
    "\n",
    "        if span:\n",
    "            span._.numero = numero\n",
    "            span._.titulo = titulo\n",
    "            span._.longitud = len(span)\n",
    "            spans.append(span)\n",
    "\n",
    "    # Guardar la lista en el atributo del Doc\n",
    "    doc._.capitulos = spans\n",
    "\n",
    "    return doc\n",
    "nlp.add_pipe(\"fragmentador_capitulos\", last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535d412",
   "metadata": {},
   "source": [
    "# Ejercicio: Segmentación en capítulos del tercer libro de Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297823c",
   "metadata": {},
   "source": [
    "- Define la variable `nlp` con el modelo `es_core_news_md`.\n",
    "- Añade el componente que has definirdo al pipeline de spaCy. Respecto a este pipeline, en el resto de esta notebook no será necesario utilizar el NER predefinido, ya que nuestro interés será detectar otro tipo de entidades que nosotros mismos vamos a definir. \n",
    "- Carga el tercer libro de Harry Potter a Partir de la línea 37. \n",
    "- Procesa el texto del tercer libro de Harry Potter con `nlp`, y comprueba que el `Doc` resultante contenga todos los capítulos segmentados y anotados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59745c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spacy.tokens.Doc.has_extension(\"capitulos\"):\n",
    "    spacy.tokens.Doc.set_extension(\"capitulos\", default=[])\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "nlp.add_pipe(\"fragmentador_capitulos\", last=True)\n",
    "with open(\"libro3.txt\", \"r\", encoding=\"utf-8\") as archivo:\n",
    "    lineas = archivo.readlines()[37:]  # Desde la línea 38\n",
    "    texto = \"\".join(lineas)\n",
    "doc = nlp(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d392399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capítulo 1: Lechuzas mensajeras (Longitud: 4418 caracteres)\n",
      "Capítulo 2: El error de tía Marge (Longitud: 4651 caracteres)\n",
      "Capítulo 3: El autobús noctámbulo (Longitud: 5433 caracteres)\n",
      "Capítulo 4: El Caldero Chorreante (Longitud: 6465 caracteres)\n",
      "Capítulo 5: El dementor (Longitud: 8178 caracteres)\n",
      "Capítulo 6: Posos de té y garras de hipogrifo (Longitud: 8244 caracteres)\n",
      "Capítulo 7: El boggart del armario ropero (Longitud: 5464 caracteres)\n",
      "Capítulo 8: La huida de la señora gorda (Longitud: 6202 caracteres)\n",
      "Capítulo 9: La derrota (Longitud: 6388 caracteres)\n",
      "Capítulo 10: El mapa del merodeador (Longitud: 8671 caracteres)\n",
      "Capítulo 11: La Saeta de Fuego (Longitud: 6762 caracteres)\n",
      "Capítulo 12: El patronus (Longitud: 5809 caracteres)\n",
      "Capítulo 13: Gryffindor contra Ravenclaw (Longitud: 5152 caracteres)\n",
      "Capítulo 14: El rencor de Snape (Longitud: 6651 caracteres)\n",
      "Capítulo 15: La final de quidditch (Longitud: 6964 caracteres)\n",
      "Capítulo 16: La predicción de la profesora Trelawney (Longitud: 5275 caracteres)\n",
      "Capítulo 17: El perro, el gato y la rata (Longitud: 5103 caracteres)\n",
      "Capítulo 18: Lunático, Colagusano, Canuto y Cornamenta (Longitud: 2622 caracteres)\n",
      "Capítulo 19: El vasallo de lord Voldemort (Longitud: 6137 caracteres)\n",
      "Capítulo 20: El Beso del dementor (Longitud: 2352 caracteres)\n",
      "Capítulo 21: El secreto de Hermione (Longitud: 8708 caracteres)\n",
      "Capítulo 22: Más lechuzas mensajeras (Longitud: 5621 caracteres)\n"
     ]
    }
   ],
   "source": [
    "for capitulo in doc._.capitulos:\n",
    "    print(f\"Capítulo {capitulo._.numero}: {capitulo._.titulo} (Longitud: {capitulo._.longitud} caracteres)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9303d2",
   "metadata": {},
   "source": [
    "## Ejercicio: Detectar hechizos y añadirlos como entidades\n",
    "\n",
    "Utilizando el objeto `PhraseMatcher` de spaCy, crea un sistema que detecte menciones de hechizos en un texto y los añada como nuevas entidades. Utiliza la siguiente lista de hechizos:\n",
    "\n",
    "`hechizos = [\"Expeliarmo\", \"Alohomora\", \"Expecto patronum\", \"Lumos\"]`\n",
    "\n",
    "Realiza este ejercicio de dos formas diferentes y razona qué está ocurriendo. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e6fe2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.detectar_hechizos(doc)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Lista de hechizos\n",
    "hechizos = [\"Expeliarmo\", \"Alohomora\", \"Expecto patronum\", \"Lumos\"]\n",
    "\n",
    "# Cargar el modelo de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Crear el PhraseMatcher y agregar las frases correspondientes\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp.make_doc(hechizo) for hechizo in hechizos]\n",
    "matcher.add(\"HECHIZOS\", None, *patterns)\n",
    "\n",
    "# Función para añadir los hechizos como nuevas entidades\n",
    "@Language.component(\"detectar_hechizos\")\n",
    "def detectar_hechizos(doc):\n",
    "    doc.ents = []\n",
    "    matches = matcher(doc)\n",
    "    # Crear entidades en el doc para cada coincidencia\n",
    "    for match_id, start, end in matches:\n",
    "        span = Span(doc, start, end, label=\"HECHIZO\")\n",
    "        doc.ents = list(doc.ents) + [span]  # Añadir la nueva entidad al doc\n",
    "    return doc\n",
    "\n",
    "# Agregar el componente a la pipeline\n",
    "nlp.add_pipe('detectar_hechizos', last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139d900",
   "metadata": {},
   "source": [
    "1. Con el `texto` del tercer libro cargado tal cual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb0d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidad: Lumos, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n"
     ]
    }
   ],
   "source": [
    "with open(\"libro3.txt\", \"r\", encoding=\"utf-8\") as archivo:\n",
    "    lineas = archivo.readlines()[37:]  # Desde la línea 38\n",
    "    texto = \"\".join(lineas)\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Ver las entidades detectadas\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"HECHIZO\":  # Filtrar solo las entidades de tipo 'HECHIZO'\n",
    "        print(f\"Entidad: {ent.text}, Etiqueta: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989876ba",
   "metadata": {},
   "source": [
    "2. Haciendo la modificación `texto = texto.replace(\"\", \" \").replace(\"\\x97\", \" \")` después de cargar el tercer libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af108da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidad: Lumos, Etiqueta: HECHIZO\n",
      "Entidad: Lumos, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Lumos, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Lumos, Etiqueta: HECHIZO\n",
      "Entidad: Expeliarmo, Etiqueta: HECHIZO\n",
      "Entidad: Expeliarmo, Etiqueta: HECHIZO\n",
      "Entidad: Expeliarmo, Etiqueta: HECHIZO\n",
      "Entidad: Expeliarmo, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Expecto patronum, Etiqueta: HECHIZO\n",
      "Entidad: Alohomora, Etiqueta: HECHIZO\n"
     ]
    }
   ],
   "source": [
    "with open(\"libro3.txt\", \"r\", encoding=\"utf-8\") as archivo:\n",
    "    lineas = archivo.readlines()[37:]  # Desde la línea 38\n",
    "    texto = \"\".join(lineas)\n",
    "texto = texto.replace(\"\", \" \").replace(\"\\x97\", \" \")\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Ver las entidades detectadas\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"HECHIZO\":  # Filtrar solo las entidades de tipo 'HECHIZO'\n",
    "        print(f\"Entidad: {ent.text}, Etiqueta: {ent.label_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
