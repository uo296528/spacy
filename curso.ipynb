{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡\n",
      "Hola\n",
      "Mundo\n",
      "!\n",
      "Mundo\n",
      "Mundo!\n",
      "Índice:    [0, 1, 2, 3, 4]\n",
      "Texto:     ['Eso', 'cuesta', '€', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Importa spaCy\n",
    "import spacy\n",
    "\n",
    "# Crea un objeto nlp vacío para procesar español\n",
    "nlp = spacy.blank(\"es\")\n",
    "# Creado al procesar un string de texto con el objeto nlp\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Itera sobre los tokens en un Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Usa el índice del Doc para obtener un solo Token\n",
    "token = doc[2]\n",
    "\n",
    "# Obtén el texto del token a través del atributo .text\n",
    "print(token.text)\n",
    "\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Un slice de un Doc en un objeto Span\n",
    "span = doc[2:4]\n",
    "\n",
    "# Obtén el texto del span a través del atributo .text\n",
    "print(span.text)\n",
    "\n",
    "doc = nlp(\"Eso cuesta €5.\")\n",
    "print(\"Índice:   \", [token.i for token in doc])\n",
    "print(\"Texto:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para empezar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n",
      "This is a sentence.\n",
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Español\n",
    "# Importa spaCy\n",
    "import spacy\n",
    "\n",
    "# Crea el objeto nlp para procesar español\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"¿Cómo estás?\")\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)\n",
    "\n",
    "# Inglés\n",
    "# Importa spaCy\n",
    "import spacy\n",
    "\n",
    "# Crea el objeto nlp para procesar inglés\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Procesa un texto (aquí dice \"Esta es una oración\" en inglés)\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)\n",
    "# Alemán\n",
    "# Importa spaCy\n",
    "import spacy\n",
    "\n",
    "# Crea el objeto nlp para procesar alemán\n",
    "nlp = spacy.blank('de')\n",
    "\n",
    "# Procesa un texto (aquí dice \"Saludos cordiales!\" en alemán)\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos, spans y tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me\n",
      "panteras negras\n",
      "panteras negras y los leones\n"
     ]
    }
   ],
   "source": [
    "# Paso 1\n",
    "# Importa spaCy y crea el objeto nlp para procesar español\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Selecciona el primer token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Imprime en pantalla el texto del token\n",
    "print(first_token.text)\n",
    "\n",
    "# Paso 2\n",
    "\n",
    "# Importa spaCy y crea el objeto nlp para procesar español\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Un slice del Doc para \"panteras negras\"\n",
    "panteras_negras = doc[3:5]\n",
    "print(panteras_negras.text)\n",
    "\n",
    "# Un slice del Doc para \"panteras negras y los leones\" (sin el \".\")\n",
    "panteras_negras_y_leones = doc[3:8]\n",
    "print(panteras_negras_y_leones.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos léxicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje encontrado: 60\n",
      "Porcentaje encontrado: 4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\n",
    "    \"En 1990, más del 60 % de las personas en Asia del Este se encontraban \"\n",
    "    \"en extrema pobreza. Ahora, menos del 4 % lo están.\"\n",
    ")\n",
    "\n",
    "# Itera sobre los tokens en el doc\n",
    "for token in doc:\n",
    "    # Revisa si el token parece un número\n",
    "    if token.like_num:\n",
    "        # Obtén el próximo token en el documento\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Revisa si el texto del siguiente token es igual a '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Porcentaje encontrado:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ella PRON\n",
      "comió VERB\n",
      "pizza PROPN\n",
      "Ella PRON nsubj comió\n",
      "comió VERB ROOT comió\n",
      "pizza PROPN obj comió\n",
      "Apple ORG\n",
      "EE.UU. LOC\n",
      "iPhone MISC\n",
      "Galaxy Note 9 MISC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python -m spacy download es_core_news_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Carga el pipeline pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Ella comió pizza\")\n",
    "\n",
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    # Imprime en pantalla el texto y la etiqueta gramatical predicha\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\n",
    "    \"Apple es la marca que más satisfacción genera en EE.UU., \"\n",
    "    \"pero el iPhone, fue superado por el Galaxy Note 9\"\n",
    ")\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "spacy.explain(\"LOC\")\n",
    "spacy.explain(\"NNP\")\n",
    "spacy.explain(\"MISC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De acuerdo con la revista Fortune, Apple fue la empresa más admirada en el mundo entre 2008 y 2012.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carga el modelo \"es_core_news_sm\"\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediciendo anotaciones lingüísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De          ADP       case      \n",
      "acuerdo     NOUN      fixed     \n",
      "con         ADP       fixed     \n",
      "la          DET       det       \n",
      "revista     NOUN      obl       \n",
      "Fortune     PROPN     appos     \n",
      ",           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "fue         AUX       cop       \n",
      "la          DET       det       \n",
      "empresa     NOUN      ROOT      \n",
      "más         ADV       advmod    \n",
      "admirada    ADJ       amod      \n",
      "en          ADP       case      \n",
      "el          DET       det       \n",
      "mundo       NOUN      obl       \n",
      "entre       ADP       case      \n",
      "2008        NOUN      nmod      \n",
      "y           CCONJ     cc        \n",
      "2012        NOUN      conj      \n",
      ".           PUNCT     punct     \n",
      "revista Fortune ORG\n",
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "# Parte 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Obtén el texto del token, el part-of-speech tag y el dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # Esto es solo por formato\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "\n",
    "# Parte 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su etiqueta\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediciendo entidades nombradas en contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olímpicos de Tokio 2020 MISC\n",
      "adidas zx ORG\n",
      "Entidad faltante: adidas zx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "# Itera sobre las entidades\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su etiqueta\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Obtén el span para \"adidas zx\"\n",
    "adidas_zx = doc[14:16]\n",
    "\n",
    "# Imprime en pantalla el texto del span\n",
    "print(\"Entidad faltante:\", adidas_zx.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando patrones basados en reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adidas zx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Importa el Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Carga el modelo y crea un objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "matcher.add(\"ADIDAS_PATTERN\", [pattern])\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matches:\n",
    "    # Obtén el span resultante\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"copa\"},\n",
    "    {\"LOWER\": \"mundial\"},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "doc = nlp(\"2014 Copa Mundial FIFA: Alemania ganó!\")\n",
    "\n",
    "\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"comer\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"Camila prefería comer sushi. Pero ahora está comiendo pasta.\")\n",
    "\n",
    "\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"comprar\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # opcional: encuentra 0 o 1 ocurrencias\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"Me compré un smartphone. Ahora le estoy comprando aplicaciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando el Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: ['adidas zx']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Importa el Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Crea un patrón que encuentre dos tokens: \"adidas\" y \"zx\"\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "matcher.add(\"ADIDAS_ZX_PATTERN\", [pattern])\n",
    "\n",
    "# Usa al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "print(\"Resultados:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describiendo patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: iOS 7\n",
      "Resultado encontrado: iOS 11\n",
      "Resultado encontrado: iOS 10\n",
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: descargué Fortnite\n",
      "Resultado encontrado: descargando Minecraft\n",
      "Resultado encontrado: descargar Winzip\n",
      "Total de resultados encontrados: 4\n",
      "Resultado encontrado: gigante tecnológico\n",
      "Resultado encontrado: lecciones virtuales\n",
      "Resultado encontrado: tecnologías avanzadas\n",
      "Resultado encontrado: tecnologías avanzadas gratuitas\n"
     ]
    }
   ],
   "source": [
    "# Parte 1\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "doc = nlp(\n",
    "    \"Después de hacer la actualización de iOS no notarás un rediseño \"\n",
    "    \"radical del sistema: no se compara con los cambios estéticos que \"\n",
    "    \"tuvimos con el iOS 7. La mayoría de las funcionalidades del iOS 11 \"\n",
    "    \"siguen iguales en el iOS 10.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para las versiones de iOS enteras\n",
    "# (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)\n",
    "\n",
    "# Parte 2\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"descargué Fortnite en mi computadora, pero no puedo abrir el juego. \"\n",
    "    \"Ayuda? Cuando estaba descargando Minecraft, conseguí la versión de Windows \"\n",
    "    \"donde tiene una carpeta '.zip' y usé el programa por defecto para \"\n",
    "    \"descomprimirlo…así que también tengo que descargar Winzip?\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón que encuentre una forma de \"descargar\" más un nombre propio\n",
    "pattern = [{\"LEMMA\": \"descargar\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)\n",
    "\n",
    "# Parte 3\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"El gigante tecnológico IBM está ofreciendo lecciones virtuales \"\n",
    "    \"sobre tecnologías avanzadas gratuitas en español.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para un sustantivo más uno o dos adjetivos\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"NOUN_ADJ_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 32833993555699147\n",
      "string value: café\n",
      "hash value: 32833993555699147\n",
      "café 32833993555699147 True\n"
     ]
    }
   ],
   "source": [
    "nlp.vocab.strings.add(\"café\")\n",
    "cafe_hash = nlp.vocab.strings[\"café\"]\n",
    "cafe_string = nlp.vocab.strings[cafe_hash]\n",
    "# Arroja un error si no hemos visto el string antes\n",
    "string = nlp.vocab.strings[32833993555699147]\n",
    "\n",
    "oc = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"café\"])\n",
    "print(\"string value:\", nlp.vocab.strings[32833993555699147])\n",
    "\n",
    "doc = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"café\"])\n",
    "\n",
    "doc = nlp(\"Ines toma café\")\n",
    "lexeme = nlp.vocab[\"café\"]\n",
    "\n",
    "# Imprime en pantalla los atributos léxicos\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De strings a hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9565357104409163886\n",
      "gato\n",
      "4317129024397789502\n",
      "PER\n"
     ]
    }
   ],
   "source": [
    "# parte 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Yo tengo un gato\")\n",
    "\n",
    "# Busca el hash para la palabra \"gato\"\n",
    "gato_hash = nlp.vocab.strings[\"gato\"]\n",
    "print(gato_hash)\n",
    "\n",
    "# Busca el gato_hash para obtener el string\n",
    "gato_string = nlp.vocab.strings[gato_hash]\n",
    "print(gato_string)\n",
    "\n",
    "# parte 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"David Bowie tiene el label PER\")\n",
    "\n",
    "# Busca el hash para el label del string \"PER\"\n",
    "person_hash = nlp.vocab.strings[\"PER\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Busca el person_hash para obtener el string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc, Span y Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un objeto nlp\n",
    "import spacy\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "# Importa las clases Doc y Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Crea un span manualmente\n",
    "span = Span(doc, 1, 3)\n",
    "\n",
    "# Crea un span con un label\n",
    "span_with_label = Span(doc, 1, 3, label=\"SALUDO\")\n",
    "\n",
    "# Añade el span a los doc.ents\n",
    "doc.ents = [span_with_label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un Doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy es divertido!\n",
      "¡Vamos, empieza!\n",
      "¡¿En serio?!\n"
     ]
    }
   ],
   "source": [
    "# parte 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# El texto deseado: \"spaCy es divertido!\"\n",
    "words = [\"spaCy\", \"es\", \"divertido\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "# parte 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# El texto deseado: \"¡Vamos, empieza!\"\n",
    "words = [\"¡\", \"Vamos\", \",\", \"empieza\", \"!\"]\n",
    "spaces = [False, False, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "# parte 3\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# El texto deseado: \"¡¿En serio?!\"\n",
    "words = [\"¡\", \"¿\", \"En\", \"serio\", \"?\", \"!\"]\n",
    "spaces = [False, False, True, False, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc, spans y entidades desde 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me gusta David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Importa las clases Doc y Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"Me\", \"gusta\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Crea un span para \"David Bowie\" a partir del doc y asígnalo al label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Añade el span a las entidades del doc\n",
    "doc.ents = [span]\n",
    "\n",
    "# Imprime en pantalla el texto y los labels de las entidades\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buenas prácticas en las estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontré un nombre propio antes de un verbo: Berlín\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Por Berlín fluye el río Esprea.\")\n",
    "\n",
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    # Revisa si el token actual es un nombre propio\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Revisa si el siguiente token es un verbo\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Encontré un nombre propio antes de un verbo:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors y similitud semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carga uno de los modelos más grandes que contiene vectores\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes_core_news_md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compara dos documentos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m doc1 \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMe gusta la comida rápida\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\ARIT\\spacy\\.venv\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\ARIT\\spacy\\.venv\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Carga uno de los modelos más grandes que contiene vectores\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Compara dos documentos\n",
    "doc1 = nlp(\"Me gusta la comida rápida\")\n",
    "doc2 = nlp(\"Me gusta la pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compara dos tokens\n",
    "doc = nlp(\"Me gustan la pizza y las hamburguesas\")\n",
    "token1 = doc[3]\n",
    "token2 = doc[6]\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "# Compara un documento con un token\n",
    "doc = nlp(\"Me gusta la pizza\")\n",
    "token = nlp(\"jabón\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "# Compara un span con un documento\n",
    "span = nlp(\"Me gustan los perros calientes\")[3:5]\n",
    "doc = nlp(\"McDonalds vende hamburguesas\")\n",
    "\n",
    "print(span.similarity(doc))\n",
    "\n",
    "# Carga uno de los modelos más grandes que contiene vectores\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\"Tengo una manzana\")\n",
    "# Accede al vector a través del atributo token.vector\n",
    "print(doc[2].vector)\n",
    "\n",
    "doc1 = nlp(\"Me gustan los gatos\")\n",
    "doc2 = nlp(\"Me desagradan los gatos\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspeccionando los word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carga el modelo es_core_news_md\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Hoy hice pan de banano\")\n",
    "\n",
    "# Obtén el vector para el token \"banano\"\n",
    "banano_vector = doc[4].vector\n",
    "print(banano_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediciendo similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parte 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc1 = nlp(\"Es un cálido día de verano\")\n",
    "doc2 = nlp(\"Hay sol afuera\")\n",
    "\n",
    "# Obtén la similitud entre el doc1 y el doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)\n",
    "\n",
    "# parte 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\"TV y libros\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Obtén la similitud entre los tokens \"TV\" y \"libros\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)\n",
    "# parte 3\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"Estuvimos en un restaurante genial. Luego, fuimos a un bar muy divertido.\"\n",
    ")\n",
    "\n",
    "# Crea los spans para \"restaurante genial\" y \"bar muy divertido\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[11:14]\n",
    "\n",
    "# Obtén la similitud entre los dos spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinando modelos y reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa con el vocabulario compartido\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Los patrones son listas de diccionarios que describen los tokens\n",
    "pattern = [{\"LEMMA\": \"comer\", \"POS\": \"VERB\"}, {\"LOWER\": \"pizza\"}]\n",
    "matcher.add(\"PIZZA\", [pattern])\n",
    "\n",
    "# Los operadores pueden especificar qué tan seguido puede\n",
    "# ser buscado un token\n",
    "pattern = [{\"TEXT\": \"muy\", \"OP\": \"+\"}, {\"TEXT\": \"feliz\"}]\n",
    "matcher.add(\"MUY_FELIZ\", [pattern])\n",
    "\n",
    "# Llamar al matcher sobre un doc devuelve una lista de\n",
    "# tuples con (match_id, inicio, final)\n",
    "doc = nlp(\"Me gusta comer pizza y estoy muy muy feliz\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERRO\", [[{\"LOWER\": \"labrador\"}, {\"LOWER\": \"dorado\"}]])\n",
    "doc = nlp(\"Tengo un labrador dorado\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"span encontrado:\", span.text)\n",
    "    # Obtén el token raíz del span y el token raíz cabeza (head)\n",
    "    print(\"Token raíz:\", span.root.text)\n",
    "    print(\"Token raíz cabeza:\", span.root.head.text)\n",
    "    # Obtén el token anterior y su POS tag\n",
    "    print(\"Token anterior:\", doc[start - 1].text, doc[start - 1].pos_)\n",
    "\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"labrador dorado\")\n",
    "matcher.add(\"PERRO\", [pattern])\n",
    "doc = nlp(\"Tengo un labrador dorado\")\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Obtén el span resultante\n",
    "    span = doc[start:end]\n",
    "    print(\"span resultante:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging de patrones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Cuando pac-man debutó en Tokio, en 1980, nadie podría haber predicho \"\n",
    "    \"que se convertiría en el videojuego más exitoso de todos los tiempos. \"\n",
    "    \"Hoy, 40 años después, aun sigue sorprendiendo. Su desarrolladora, \"\n",
    "    \"Bandai Namco, ha anunciado novedades en el marco del aniversario del \"\n",
    "    \"juego. La celebración del 40 aniversario de pac-Man en 2020 incluirá \"\n",
    "    \"el debut de una nueva canción temática, compuesta por el famoso artista \"\n",
    "    \"japonés de Techno Ken Ishii. Además de estas novedades, Bandai Namco \"\n",
    "    \"publicará nuevas versiones del videojuego. La primera será pac-man Live \"\n",
    "    \"Studio, en Twitch, en colaboración con Amazon Games.\"\n",
    ")\n",
    "\n",
    "# Crea los patrones\n",
    "pattern1 = [{\"LIKE_NUM\": True}, {\"POS\": \"NOUN\"}]\n",
    "pattern2 = [{\"LOWER\": \"pac-man\"}, {\"IS_TITLE\": True}]\n",
    "\n",
    "# Inicializa el Matcher y añade los patrones\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Imprime en pantalla el nombre en string del patrón\n",
    "    # y el texto del span encontrado\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando frases eficientemente, 'phrase matching'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"exercises/es/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "doc = nlp(\n",
    "    \"La Unión Europea fue fundada por seis países de Europa occidental \"\n",
    "    \"(Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y \"\n",
    "    \"se amplió en seis ocasiones.\"\n",
    ")\n",
    "\n",
    "# Importa el PhraseMatcher e inicialízalo\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Crea objetos Doc patrón y añádelos al matcher\n",
    "# Esta es una versión más rápida de: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Llama al matcher sobre el documento de prueba e imprime el resultado en pantalla\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrayendo países y relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"exercises/es/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/es/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Crea un doc y restablece las entidades existentes\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Crea un Span con la etiqueta para \"LOC\"\n",
    "    span = Span(doc, start, end, label=\"LOC\")\n",
    "\n",
    "    # Sobrescribe el doc.ents y añade el span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Obtén el token cabeza de la raíz del span\n",
    "    span_root_head = span.root.head\n",
    "    # Imprime en pantalla el texto del token cabeza de\n",
    "    # la raíz del span y el texto del span\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Imprime en pantalla las entidades del documento\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001BC186B8D00>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x000001BC18818D60>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001BC1872F890>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001BC1881BC40>), ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer object at 0x000001BC1880FF00>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001BC1872F820>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carga el modelo es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Imprime en pantalla los nombres de los componentes del pipeline\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Imprime en pantalla el pipeline entero de tuples (name, component)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes personalizados del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Haz algo con el doc aquí\n",
    "    return doc\n",
    "\n",
    "#nlp.add_pipe(custom_component)\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Haz algo con el doc aquí\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"custom_component\")\n",
    "\n",
    "# Crea el objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define un componente personalizado\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Imprime los nombres de los componentes del pipeline\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Crea el objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define un componente personalizado\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"¡Hola Mundo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Obtén la longitud del doc\n",
    "    doc_length = len(doc)\n",
    "    print(f\"Este documento tiene {doc_length} tokens.\")\n",
    "    # Devuelve el doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Carga el modelo pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Añade el componente en el primer lugar del pipeline e imprime\n",
    "# los nombres de los pipes en pantalla\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Esto es una frase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes complejos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "animals = [\"labrador dorado\", \"gato\", \"tortuga\", \"oso de anteojos\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"patrones_de_animales:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Aplica el matcher al doc\n",
    "    matches = matcher(doc)\n",
    "    # Crea un Span para cada resultado y asígnales el label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescribe los doc.ents con los spans resultantes\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline después del componente \"ner\"\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto y el label\n",
    "# de los doc.ents\n",
    "doc = nlp(\"Hoy vimos una tortuga y un oso de anteojos en nuestra caminata\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensión de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las clases globales\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Añade extensiones para el Doc, Token y Span\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Añade una extensión en el Token con un valor por defecto\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "\n",
    "# Sobrescribe el valor de la extensión de atributo\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Define la función getter\n",
    "def get_is_color(token):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Añade una extensión en el Token con getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Define la función getter\n",
    "def get_has_color(span):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Añade una extensión en el Span con getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define un método con argumentos\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Añade una extensión en el Doc con el método\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc._.has_token(\"azul\"), \"- azul\")\n",
    "print(doc._.has_token(\"nube\"), \"- nube\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadiendo extensiones de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Registra la extensión de atributo del Token, \"is_country\",\n",
    "# con el valor por defecto False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Procesa el texto y pon True para el atributo \"is_country\"\n",
    "# para el token \"España\"\n",
    "doc = nlp(\"Vivo en España.\")\n",
    "doc[2]._.is_country = True\n",
    "\n",
    "# Imprime en pantalla el texto del token y el atributo \"is_country\"\n",
    "# para todos los tokens\n",
    "print([(token.text, token._.is_country) for token in doc])\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define la función getter que toma un token y devuelve su texto al revés\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Token, \"reversed\", con\n",
    "# el getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el atributo \"reversed\"\n",
    "# para cada token\n",
    "doc = nlp(\"Todas las generalizaciones son falsas, incluyendo esta.\")\n",
    "for token in doc:\n",
    "    print(\"invertido:\", token._.reversed)\n",
    "\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define la función getter\n",
    "def get_has_number(doc):\n",
    "    # Devuelve si alguno de los tokens en el doc devuelve True\n",
    "    # para token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Doc, \"has_number\",\n",
    "# con el getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Procesa el texto y revisa el atributo personalizado \"has_number\"\n",
    "doc = nlp(\"El museo cerró por cinco años en el 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)\n",
    "\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define el método\n",
    "def to_html(span, tag):\n",
    "    # Envuelve el texto del span en un HTML tag y devuélvelo\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Span, \"to_html\",\n",
    "# con el método \"to_html\"\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Procesa el texto y llama el método \"to_html\"en el span\n",
    "# con el nombre de tag \"strong\"\n",
    "doc = nlp(\"Hola mundo, esto es una frase.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades y extensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Obtén la URL de Wikipedia si el span tiene uno de los siguientes labels\n",
    "    if span.label_ in (\"PER\", \"ORG\", \"LOC\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://es.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Añade la extensión del Span, wikipedia_url, usando el getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Antes de finalizar 1976, el interés de David Bowie en la \"\n",
    "    \"floreciente escena musical alemana, le llevó a mudarse a \"\n",
    "    \"Alemania para revitalizar su carrera.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la URL de Wikipedia de la entidad\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes con extensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/es/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/es/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Crea un Span de entidades con el label \"LOC\" para todos los resultados\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"LOC\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# El getter que busca el texto del span en un diccionario de ciudades\n",
    "# capitales de países\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Registra la extensión de atributo del Span, \"capital\", con el \n",
    "# getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto de la entidad,\n",
    "# el label y los atributos \"capital\"\n",
    "doc = nlp(\n",
    "    \"La República Checa podría ayudar a la República Eslovaca \"\n",
    "    \"a proteger su espacio aéreo\"\n",
    ")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentando la escala y desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Esto es un texto\", {\"id\": 1, \"numero_pagina\": 15}),\n",
    "    (\"y otro texto\", {\"id\": 2, \"numero_pagina\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"numero_pagina\"])\n",
    "\n",
    "# Desactiva el tagger y el parser\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Procesa el texto e imprime las entidades en pantalla\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesando streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "with open(\"exercises/es/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Procesa los textos e imprime los verbos en pantalla\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "with open(\"exercises/es/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Procesa los textos e imprime las entidades en pantalla\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Crea una lista de patrones para el PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesando datos con contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/es/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "nlp = Spanish()\n",
    "\n",
    "# Registra la extensión del Doc, \"author\" (por defecto None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "# Registra la extensión del Doc, \"book\" (por defecto None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Añade los atributos doc._.book y doc._.author desde el contexto\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Imprime en pantalla el texto y los datos del atributo personalizado\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento selectivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A es una cadena de restaurantes de comida rápida \"\n",
    "    \"americana con sede en la ciudad de College Park, Georgia, \"\n",
    "    \"especializada en sándwiches de pollo.\"\n",
    ")\n",
    "\n",
    "# Únicamente convierte el texto en tokens\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A es una cadena de restaurantes de comida rápida \"\n",
    "    \"americana con sede en la ciudad de College Park, Georgia, \"\n",
    "    \"especializada en sándwiches de pollo.\"\n",
    ")\n",
    "\n",
    "# Deshabilita el parser\n",
    "with nlp.select_pipes(disable=[\"parser\"]):\n",
    "    # Procesa el texto\n",
    "    doc = nlp(text)\n",
    "    # Imprime las entidades del doc en pantalla\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando un modelo de red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando y actualizando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.tokens import DocBin, Span\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Crea un Doc con spans de entidades\n",
    "doc1 = nlp(\"el iPhone X está por salir\")\n",
    "doc1.ents = [Span(doc1, 1, 2, label=\"GADGET\")]\n",
    "# Crea otro Doc sin spans de entidades\n",
    "doc2 = nlp(\"¡Necesito un nuevo teléfono! ¿Alguien tiene recomendaciones?\")\n",
    "\n",
    "docs = [doc1, doc2]  # y así sucesivamente...\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]\n",
    "\n",
    "# Crea y guarda una colección de docs para entrenamiento\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "# Crea y guarda una colección de docs para evaluación\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "with open(\"exercises/es/adidas.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Dos tokens que en minúsculas encuentran \"adidas\" y \"zx\"\n",
    "pattern1 = [{\"LOWER\": \"adidas\"}, {\"LOWER\": \"zx\"}]\n",
    "\n",
    "# Token que en minúsculas encuentra \"adidas\" y un dígito\n",
    "pattern2 = [{\"LOWER\": \"adidas\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Añade los patrones al matcher y revisa el resultado\n",
    "matcher.add(\"ROPA\", [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "with open(\"exercises/es/adidas.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Agrega patrones al matcher\n",
    "pattern1 = [{\"LOWER\": \"adidas\"}, {\"LOWER\": \"zx\"}]\n",
    "pattern2 = [{\"LOWER\": \"adidas\"}, {\"IS_DIGIT\": True}]\n",
    "matcher.add(\"ROPA\", [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)\n",
    "\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar y correr el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: es\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./config.cfg --lang es --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!cat ./config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo utilizar el CLI de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ./exercises/en/config_gadget.cfg --output ./output --paths.train ./exercises/en/train_gadget.spacy --paths.dev ./exercises/en/dev_gadget.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buenas prácticas para entrenar modelos de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "doc1 = nlp(\"El año pasado fuí a Venecie y los canales estaban hermosos\")\n",
    "doc1.ents = [Span(doc1, 5, 6, label=\"LOC\")]\n",
    "\n",
    "doc2 = nlp(\"Deberías visitar Madrid una vez en tu vida, \"\n",
    "           \"pero el museo prado es aburrido\")\n",
    "doc2.ents = [Span(doc2, 2, 3, label=\"LOC\"),\n",
    "             Span(doc2, 11, 13, label=\"LOC\")]\n",
    "\n",
    "doc3 = nlp(\"Yo sé que también hay un Madrid en Colombia, jaja\")\n",
    "doc3.ents = [Span(doc3, 6, 7, label=\"LOC\"), Span(doc3, 8, 9, label=\"LOC\")]\n",
    "\n",
    "doc4 = nlp(\"Una ciudad como Berlín es perfecta para las vacaciones de verano: \"\n",
    "           \"muchos, parques, gran vida nocturna, cerveza barata!\")\n",
    "doc4.ents = [Span(doc4, 3, 4, label=\"LOC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "doc1 = nlp(\"Reddit hizo una alianza con Patreon para ayudar a los creadores\",)\n",
    "doc1.ents = [\n",
    "    Span(doc1, 0, 1, label=\"WEBSITE\"),\n",
    "    Span(doc1, 5, 6, label=\"WEBSITE\"),\n",
    "]\n",
    "\n",
    "doc2 = nlp(\"PewDiePie rompe el record de YouTube\")\n",
    "doc2.ents = [Span(doc2, 5, 6, label=\"WEBSITE\")]\n",
    "\n",
    "doc3 = nlp(\"El fundador de Reddit, Alexis Ohanian, regaló dos tiquetes de Metallica\")\n",
    "doc3.ents = [Span(doc3, 3, 4, label=\"WEBSITE\")]\n",
    "\n",
    "\n",
    "#multietiqueta\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "doc1 = nlp(\"Reddit hizo una alianza con Patreon para ayudar a los creadores\",)\n",
    "doc1.ents = [\n",
    "    Span(doc1, 0, 1, label=\"WEBSITE\"),\n",
    "    Span(doc1, 5, 6, label=\"WEBSITE\"),\n",
    "]\n",
    "\n",
    "doc2 = nlp(\"PewDiePie rompe el record de YouTube\")\n",
    "doc2.ents = [Span(doc2, 0, 1, label=\"PER\"), Span(doc2, 5, 6, label=\"WEBSITE\")]\n",
    "\n",
    "doc3 = nlp(\"El fundador de Reddit, Alexis Ohanian, regaló dos tiquetes de Metallica\")\n",
    "doc3.ents = [Span(doc3, 3, 4, label=\"WEBSITE\"), Span(doc3, 5, 7, label=\"PER\")]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
